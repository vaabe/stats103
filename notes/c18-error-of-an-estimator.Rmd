---
title: 'chapter 18 Quantifying the Error of an Estimator'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
library(tufte)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\MSE}{\text{MSE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

\renewcommand{\v}[1]{{\boldsymbol #1}}
\renewcommand{\dv}[1]{\dot{\boldsymbol{#1}}}
\newcommand{\ddv}[1]{\ddot{\boldsymbol{#1}}}
\newcommand{\hv}[1]{\hat{\boldsymbol{#1}}}
\newcommand{\m}[1]{[ #1 ]}
\renewcommand{\t}[1]{\widetilde{\boldsymbol{#1}}}
\newcommand{\bfit}[1]{\textbf{\textit{#1}}}

---

# 18--1 The Quality of an Estimator 

How good is an estimator overall? If $\hat\theta$ is an estimator for $\theta$, one way to quantify the *overall* discrepancy between $\hat\theta$ and $\theta$ is using a **loss function**, $L(\theta, \hat\theta)$. Two examples of loss functions:

$$L(\theta, \hat\theta) = | \hat\theta - \theta | \hspace{2cm} \text{absolute error loss}$$
$$L(\theta, \hat\theta) = ( \hat\theta - \theta )^2 \hspace{2cm} \text{squared error loss}$$

To assess an estimator we can compute the *average* loss, which we denote the **risk** of an estimator: 

$$R(\theta,\hat\theta) = \E \big[ L(\theta,\hat\theta) \big]$$

## The mean squared error

A common measure of an estimator's overall quality is the mean squared error, MSE, defined:

$$\MSE(\hat\theta) = \E \big[ (\hat\theta - \theta)^2 \big]$$

i.e. the MSE is the expected value of the estimator's squared error loss---it's a measure of the average squared distance between the estimator and the true value.  

We know that bias and variance both contribute to the overall error of an estimator. By expanding the form given above, it's possible to show that the MSE is a combination of both:

$$
\begin{aligned}
  \MSE(\hat\theta) &= \E \big[ (\hat\theta - \theta)^2 \big] \\ 
  &= \E \big[ (\hat\theta - \E[\hat\theta] + \E[\hat\theta] - \theta)^2] \\ 
  &\; \; \vdots \\ 
  &= \E \big[(\hat\theta - \E[\hat\theta])^2] + \big( \E[\hat\theta]-\theta \big)^2 \\ 
  &= \Var[\hat\theta] + \Bias[\hat\theta]^2
\end{aligned}
$$

`r margin_note("See the full proof of the Bias-Variance decomposition of MSE <a href='https://en.wikipedia.org/wiki/Mean_squared_error#Proof_of_variance_and_bias_relationship'>here</a>.")`

i.e. the MSE can be expressed as the sum of the variance and squared bias of the estimator. If an estimator is unbiased, its MSE is simply equal to its variance.  

## Trading between bias and variance

Note the difference between bias and variance:

$$\Var[\hat\theta] = \E \big[ (\hat\theta - \E[\hat\theta])^2 \big] \hspace{1cm} \Bias[\hat\theta] = \E[\hat\theta] - \theta$$

i.e. variance is a measure of the *spread* of an estimate (how closely the data points are clustered), and bias is a measure of how far the cluster actually is from the true value. Below is an illustration of different combinations of bias and variance:  

```{r, echo=FALSE, out.height=400, out.width=400, fig.align='center', fig.cap="Source: http://scott.fortmann-roe.com/docs/BiasVariance.html"}
knitr::include_graphics('./pics/c18-pic1.png')
```

The *ideal* estimator would have low bias and low variance. It turns out though that it's not always possible to minimize both---there are many cases where an estimator with low variance has high bias, and vice versa. You'll come to see how sometimes it's more optimal to introduce a little bias, as this can result in a better overall estimator.   

Choosing the ideal estimator is part of **statistical decision theory**.  



\ 

# 18--2 The Gauss-Markov Theorem 

The Gauss-Markov theorem says that in a regression model with homoscedastic errors, the least squares estimator is BLUE (the Best Linear Unbiased Estimator). It's dubbed the "best" estimator since it's *unbiased* and also has the smallest possible variance (among all other unbiased estimators).   

## Proof of unbiasedness

The multivariate LS coefficient vector is given (from <a href="https://stats103.org/notes/c15-regression-theory.html#multivariate-derivation">chapter 15</a>):

$$\v b = (\v X^T \v X)^{-1} \v X^T \v y$$

Substituting $\v y = \v X \v \beta + \v \varepsilon$:

$$
\begin{aligned}
  \v b &= (\v X^T \v X)^{-1} \v X^T (\v X \v \beta + \v \varepsilon) \\ 
  &= \v \beta + (\v X^T \v X)^{-1} \v X^T \v \varepsilon
\end{aligned}
$$

Thus:

$$\E[\v b | \v X] = \v \beta + \E \big[ (\v X^T \v X)^{-1} \v X^T \v \varepsilon | \v X \big]$$

But one of the assumptions of least squares regression is that the residuals are uncorrelated with the predictors, i.e. $\v \varepsilon | \v X = 0$, which makes the second term go to zero, leaving

$$\E[\v b | \v X] = \v \beta$$

i.e. the LS estimator $\v b$ is an unbiased estimate of $\v \beta$.  

**Caveat:** the LS estimator gives unbiased estimates of the true regression coefficients *in theory*, but the unbiasedness is not so obvious if the data is a single sample of a population. You also have to look at the sampling strategy and the experiment design---if the sample is biased then the estimates will be biased by default.  

## Proof of minimum variance

See <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem#Proof">this link</a>. It shows that the LS estimator is has the lowest possible variance among all linear unbiased estimators.  

However the LS estimator is not the minimum variance estimator in general---it turns out there is a *biased* estimator with an even smaller variance, known as the **James-Stein estimator** (next).  





\ 

# 18--3 Stein's Paradox

Suppose the parameters of interest are the mean of a multivariate normal $\theta = \mu = (\mu_1, ..., \mu_p)$.  

Suppose $X$ is multivariate normal with mean $\mu$. Thus $E[X] = \mu$ is unbiased. What about MSE?

If $p=1$ or $p=2$, $X$ has the lowest MSE.  

If $p \geq 3$ then $X$ no longer has the lowest MSE.  

This is sometimes called Stein’s paradox after Charles Stein.

In high dimensions, it’s usually better to be biased.  

If you find this very interesting there is a classic example about baseball you can read about. Summary: when estimating many players’ batting averages in the next season, instead of using each players’ average from this season as their own estimate it’s better to make all the estimates biased toward the overall average of the players.  

```{r}
p = 100
JS <- function(x) max((1 - (p-2)/sum(x^2)),0) * x
mu = sample(c(1:5), p, replace = TRUE)
```

```{r}
SEs <- replicate(10000, sum(((rnorm(p) + mu) - mu)^2))
JSSEs <- replicate(10000, sum(((JS(rnorm(p) + mu)) - mu)^2))
mean(SEs)
```

```{r}
mean(JSSEs)
```
