---
title: 'chapter 4 Basic Data Analysis'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background']
    css: 'tufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(tufte)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2018.csv')

paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))

nycheat <- read.csv('./data/nyc-heatwave.csv')

nycbnb <- read.csv('./data/nyc-airbnb.csv', header = TRUE) %>%
  mutate(price = as.numeric(price), 
         cleaningfee = as.numeric(cleaningfee), 
         availability365 = as.numeric(availability365), 
         sqft = as.numeric(sqft))
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

\  

# 4--1 Working with Sample Data 

Every experiment or study is concerned with answering some question:

- what is the true extent of the gender pay gap?
- why is New York City insufferably hot in summer?

Conducting a study involves choosing relevant variables to answer the question, collecting empirical data on these variables, and using some method of analysis to draw inferences from the data. 

Every study has a **population**--this is the relevant set of events or observations required to fully answer the question. E.g. if trying to determine the true gender pay gap, the population might be the salary of every single person.  

A **sample** is a subset of the events/observations in a population. Most studies are conducted with samples, since it is often impossible to acquire data on a whole population. 

Below are two examples of sample data relevant to the questions raised above. 



## Concerning the gender pay gap

The following dataset has variables characterising the gender pay gap at over 10,000 UK firms in 2018.^[The data is freely available at https://gender-pay-gap.service.gov.uk/. Excuse the horribly cumbersome variable names.]  

Below is a preview of the dataset and some of the variables contained therein:

```{r, echo=FALSE}
kable(paygap[c(323,343,521,1387,1389,1495,3313,3889,4884,6214,7294,7299,10029), c(1:3,7:10)])
```



## Concerning heatwaves in New York

The following dataset has variables characterising the temperature and urban features of over 1000 locations in the city during a heatwave.^[Download the data <a href="./data/nyc-heatwave.csv" download>here</a>. This data was simulated from the findings of this <a href="base64 data" download="./downloads/nyc-heat-island-mitigation.pdf">study</a>, investigating the extreme heating effect in NYC. Additional sources: <a href="https://www.ncdc.noaa.gov/cdo-web/search">NOAA</a>, <a href="https://landsat.visibleearth.nasa.gov/view.php?id=6800">Landsat</a>, <a href="https://data.cityofnewyork.us/Environment/Landcover-Raster-Data-2010-3ft-Resolution/9auy-76zt">NYC OpenData</a>.] Each recorded location was in one of six geographical areas. Below is a preview of the dataset:

`r margin_note("A brief addendum on the variables in this dataset:")`

`r margin_note("**Temperature** is measured in $^oF$.")`

`r margin_note("**Vegetation** is a measure of the relative concentration of vegetation (i.e. greenery) in the recorded area. It is measured in NDVI (normalized difference vegetation index), a number between -1 and 1. 0 indicates no vegetation, 1 indicates maximum vegetation, and negative values indicate water.")`

`r margin_note("**Albedo** is a measure of the percentage solar reflectivity of buildings in recorded area. E.g. an albedo of 25% means buildings reflect on average 25% of solar radiation.")` 

`r margin_note("**Building height** is the average height of buildings in the recorded area. It is measured in storeys.")`



```{r, echo=FALSE}
kable(nycheat[c(1:3, 196:198, 407:409, 524:526, 807:809, 898:900), ])
```



\ 

# 4--2 Summarizing Data  

When you have a sample of data, it is useful to understand its **distribution**. As demonstrated in the previous chapter, histograms are a good way to visualize the distribution of data within a variable.  

From the pay gap data, below is a histogram of the variable `PropFemaleTopQuartile` -- the proportion of females in the top-earning quartile. 

```{r}
ggplot(aes(x = PropFemaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = ..density..)) +
  xlab('proportion of females in top-earning quartile') +
  theme_bw()
```

**Summary statistics** are point values that summarize some characteristic of a distribution--e.g. central tendency, spread, skew, etc. They can help extract meaning from a distribution. 

The `summary()` function produces a few common ones: 

```{r}
summary(paygap$PropFemaleTopQuartile)
```



## Central tendency

Measures of central tendency indicate the central or typical value of a distribution. The most common is the mean, though others are useful in certain situations.   



**Mean**

The mean of a set of observations is the sum of all observations divided by the number of observations. For sample data this statistic is known as the **sample mean**:

$$\bar X = \frac 1n \sum_i^n X_i$$

where $X_i$ is an individual observation in the set, and $n$ is the number of observations.  

In R you can use `mean()`:

```{r}
mean(paygap$PropFemaleTopQuartile)
```


**Median**

The median is the middle value or the 50th percentile of a distribution. Half the observations are below (and above) the median: 

$$m = \frac 12 \big( X_{(n/2)} + X_{(n/2+1)} \big) \hspace{0.5cm} \text{or} \hspace{0.5cm} X_{(n+1)/2)}$$

In R you can use `median()`: 

```{r}
median(paygap$PropFemaleTopQuartile)
```



**Mode**

```{r, echo=FALSE, fig.margin = TRUE, fig.cap = "The mode can be visualized as follows."}
ggplot(aes(x = PropFemaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = ..density..)) +
  xlab('proportion of females in top-earning quartile') +
  geom_vline(xintercept = 0.102, linetype = 'dashed', color = 'violet') + 
  annotate('text', x = 0.17, y = 2, label = 'Mode', color = 'violet', size=5) + 
  theme_bw()
```

The mode is the most commonly occurring value in a distribution. Although there is no function in R to formally calculate the mode, you can visualize it as the value corresponding to the peak in a histogram or density plot. 





## Dispersion 

Measures of dispersion indicate the spread or variation of a distribution. The most common is the standard deviation. 



**Standard Deviation**

The standard deviation of a set of observations is the average distance of each observation from the mean. For sample data this statistic is known as the **sample standard deviation**:^[Note the denominator in the formula is $n-1$, not $n$. This is known as Bessel's correction, and is used when calculating the standard deviation of a **sample** of data. More on this later.]

$$s = \sqrt{\frac{1}{n-1} \sum_i^n (X_i - \bar X)^2}$$

In R you can use `sd()`:^[Note the `sd()` function in R computes the sample standard deviation, i.e. it uses the $n-1$ denominator.] 

```{r}
sd(paygap$PropFemaleTopQuartile)
```

The formula for standard deviation is derived from **variance**--the average squared distance of each observation from the mean:

$$s^2 = \frac{1}{n-1} \sum_i^n (X_i - \bar X)^2$$



**Quartiles**

The **lower quartile** is the 25th percentile of the distribution (one quarter of the observations are below it). 

The **upper quartile** is the 75th percentile of the distribution (one quarter of the observations are above it). 



## Skew

In a perfectly symmetric distribution, the mean, median, and mode are equal. Disparities indicate the distribution is skewed.  

Compare the distributions for females vs males in the top-earning quartile:  

```{r, echo=FALSE, fig.width=10, fig.height=5}
plot1 <- ggplot(aes(x = PropFemaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = stat(density))) +
  xlab('proportion of females in the top-earning quartile') + 
  geom_vline(xintercept = mean(paygap$PropFemaleTopQuartile), linetype = 'dashed', color = 'red') +
  annotate('text', x = 0.47, y = 2, label = 'Mean', color = 'red') + 
  geom_vline(xintercept = median(paygap$PropFemaleTopQuartile), linetype = 'dashed', color = 'blue') + 
  annotate('text', x = 0.30, y = 2, label = 'Median', color = 'blue') + 
  geom_vline(xintercept = 0.105, linetype = 'dashed', color = 'violet') + 
  annotate('text', x = 0.17, y = 2, label = 'Mode', color = 'violet') + 
  theme_bw()

plot2 <- ggplot(aes(x = PropMaleTopQuartile), data = paygap) + 
  geom_histogram(bins = 50, aes(y = stat(density))) +
  xlab('proportion of males in the top-earning quartile') + 
  geom_vline(xintercept = mean(paygap$PropMaleTopQuartile), linetype = 'dashed', color = 'red') +
  annotate('text', x = 0.51, y = 2, label = 'Mean', color = 'red') + 
  geom_vline(xintercept = median(paygap$PropMaleTopQuartile), linetype = 'dashed', color = 'blue') + 
  annotate('text', x = 0.71, y = 2, label = 'Median', color = 'blue') + 
  geom_vline(xintercept = 0.90, linetype = 'dashed', color = 'violet') + 
  annotate('text', x = 0.83, y = 2, label = 'Mode', color = 'violet') + 
  theme_bw()

grid.arrange(plot1, plot2, ncol = 2)
```

Naturally^[Due to the cis-normativity of the study.] these distributions are mirror images of each other. 

The distribution for females has **right-skew** (or positive skew), since the tail drags rightwards (in the positive direction). The distribution for males has **left-skew** (negative skew), since the tail drags leftwards (in the negative direction).  

- under right skew, mean $>$ median $>$ mode
- under left skew, mode $>$ median $>$ mean

Formally, the **skewness** of a sample of data is defined as: 

$$g_1 = \frac{\frac 1n \sum_i^n (X_i - \bar X)^3}{s^3}$$

In R you can use `skewness()`:^[From the `moments` package.]

```{r}
skewness(paygap$PropFemaleTopQuartile)
```

The positive value indicates the distribution for females is positively skewed. Predictably, the skewness of the distribution for males is:

```{r}
skewness(paygap$PropMaleTopQuartile)
```




## Outliers 

Outliers are data points that differ substantially from other observations. The presence of outliers in a sample indicates either:

1. there is measurement error in the experiment
2. the population has a skewed/heavy-tailed distribution  

Outliers due to measurement error should be discarded as they unduly skew results. Below is an example from a classic dataset measuring the speed of light in 1879:  

```{r, echo=FALSE}
library(datasets)

ggplot(aes(x = as.factor(Expt), y = Speed + 299000), data = morley) + 
  geom_boxplot() +
  ggtitle('Michelson-Morley Experimental Data 1879') + 
  xlab('experiment no.') + ylab('speed of light (km/s)') +
  theme_bw()
```

Box and whisker plot are a good way to visualize outliers in a distribution of data. In this example, experiments 1 and 3 have outliers.  

If outliers are not due to measurement error, they may suggest the distribution is intrinsically **skewed** or **heavy-tailed**. 

In general, outliers should not be discarded unless they are obviously due to measurement error.  

\ 

**Mean vs. Median**

In the presence of outliers, the mean is more susceptible to skew than the median. This is because the mean is weighted by each observation in a sample. The median is not--it is simply the middle value.  

E.g. in the following sample, nine observations are between 21 and 22, and one outlier is above 80:

```{r, echo=FALSE}
X <- c(round(runif(n = 9, min = 21, max = 22),1), 83.4)
kable(t(X))
```

The mean of this sample is $\bar X =$ `r toString(round(mean(X), 2))` and the median is $m =$ `r toString(round(median(X), 2))`. In this case the median is clearly a more representative measure of central tendency than the mean.  

Strong outliers drag the mean in the direction of skew (in this case positive). This is why under positive skew the mean is larger than the median, and vice versa.  

Estimators are said to be **robust** if they can cope with outliers. The median is a robust measure of central tendency; the mean is not. However the mean is generally a more precise statistic.   



**Median Absolute Deviation (MAD)**

The median absolute deviation is a robust measure of dispersion. Mathematically it is defined as the median of the absolute deviations of each observation from the data's median: 

$$\text{MAD} = \text{median}(|X_i - m|)$$

where $m$ is the median of the data.  

The MAD is more resilient to outliers than the standard deviation is, since the former uses absolute deviations, while the latter uses squared deviations.  

In R, you can use `mad()`:^[From the `stats` package.] 

```{r}
mad(paygap$PropFemaleTopQuartile)
```



\ 

# 4--3 Point Statistics vs Intervals

```{r, include=FALSE}

```

A summary statistic derived from samples of data--such as the sample mean and sample standard deviation--is known as an **estimator**, since it uses a sample to estimate the true value of a population parameter.^[E.g. the *true* mean difference in hourly wages between females and males is unknown; but the sample of pay gap data can be used to estimate it.] 

The summary statistics introduced above are known as **point estimators** since they use a single value to estimate a population parameter. 

There also exist **interval estimators**, which use a *range* of values to estimate a population parameter. Often these are more useful than single-valued estimates, as they demonstrate the margin of uncertainty when drawing inferences from data.

A **confidence interval** is one example of an interval estimator.  



## A confidence interval for the mean

A confidence interval for a mean is a range of values that might contain the true mean of the population distribution. Every confidence interval is associated with a **confidence level**, which describes the approximate probability the true mean lies in the range specified.^[The theory behind this is explained in Part II.] E.g. a 95\% confidence interval for the mean is a range of values where:

$$\P(LB \leq \mu \leq UB) \approx 0.95$$

$LB$ and $UB$ are the lower and upper bounds of the interval, which have an approximately 95\% probability of containing the true mean.  

Below is a function^[Note, this is a user-defined function. Running this code will define a new function in the Environment, called $\texttt{confidence_interval}$. Once defined, this function can be used to compute a confidence interval for the mean of an array of data. For more on writing functions in R, click <a href="https://www.statmethods.net/management/userfunctions.html">here</a>.] that takes an array of data and computes a confidence interval for the mean: 

```{r}
confidence_interval = function(data, conflevel) {
  xbar = mean(data)          # sample mean 
  n = length(data)           # sample size 
  SE = sd(data) / sqrt(n)    # standard error
  alpha = 1 - conflevel      # alpha
  
  lb = xbar + qt(alpha/2, df = n-1) * SE    # calculate lower bound
  ub = xbar + qt(1-alpha/2, df = n-1) * SE  # calculate upper bound
  
  cat(paste(c('sample mean =', round(xbar,3), '\n', 
              conflevel*100, '% confidence interval:', '\n', 
              'lower bound =', round(lb,3), '\n', 
              'upper bound =', round(ub,3))))
}
```

The way this function is written, it takes two arguments: `data`--a vector array of data, and `conflevel`--the desired confidence level.  

E.g. 95\% confidence interval for the mean difference in hourly wages between females and males can be computed as follows:

```{r}
confidence_interval(data = paygap$DiffMeanHourlyPercent, conflevel = 0.95)
```

The output suggests that the true mean difference in wages is approximately 95\% likely to lie somewhere between 13.93\% and 14.47\%. 

Note there is one important caveat to this claim. A confidence interval computed from sample data (such as the one above) will always be centered on the sample mean, not the true mean. This means the *actual* probability of the interval containing the true mean depends on how close the sample mean is to the true mean. Since the true mean of a distribution is often unknown, there is no way to tell from any one sample of data whether the interval contains the true mean, or the likelihood thereof. This is why the probability associated with a confidence interval is only ever approximate--if at all.  

Nonetheless confidence intervals are still useful as they demonstrate the degree of uncertainty in an estimate, whether the estimate is accurate or not. There is more theory on confidence intervals in Part II. 



\ 

# 4--4 Simple Hypothesis Tests 

A **statistical hypothesis** is essentially an assumption about the *true* value of a population parameter. Many studies start with a hypothesis of some sort, then collect relevant sample data to assess the validity of the hypothesis.  

Hypothesis testing involves comparing two competing models that attempt to describe the true value of a population parameter. Usually one of the models is preconceived (devised by the experimenter) and the other model is empirical (based on sample of data that has been collected). In this framework the sample of data is used to either validate or reject the preconceived model.  

Formally, a hypothesis test is devised in terms of two hypotheses:

- **the null hypothesis**, $H_0$, a proposed model for a population 
- **the alternative hypothesis**, $H_1$, that the proposed model is not true 

The test is deemed **statistically significant** if the sample data gives strong evidence *against* the null hypothesis--i.e. if it's an unlikely realization of the null. This usually leads to rejecting the null hypothesis.  

Some simple tests are introduced below. There is more theory on hypothesis tests in Part II. 



## One-sample t-test for a mean 

A one-sample $t$-test compares the mean of a sample of data to some hypothesized value for the true mean. The null and alternative hypotheses are:

- $H_0$: the true mean is equal to some specified value $k$
- $H_1$: the true mean is not equal to $k$

E.g. say you hypothesize that the true mean difference in hourly wages between females and males is zero--as perhaps it ought to be. Using the pay gap data, you could test this hypothesis by calculating the mean of `DiffMeanHourlyPercent` and comparing it to your proposed value. In this test the null and alternative hypotheses are:

$$
\begin{aligned}
  H_0: \texttt{true mean wage gap} = 0 \\ 
  H_1: \texttt{true mean wage gap} \neq 0
\end{aligned}
$$

In R, you can use `t.test()` to carry out the test. The function will need two inputs: the proposed value for the true mean (this argument is denoted `mu`), and the sample of data you're using to test it: 

```{r}
t.test(mu = 0, paygap$DiffMeanHourlyPercent)
```

The observed value of this test is printed at the bottom: $\bar X =$ 14.1985. This tells you that the mean difference between female and male hourly wages is 14.1985\%, according to this sample.  

Now it is up to you whether to reject the null hypothesis or not, on the basis of this evidence. You can use the $p$-value of the test to help you make a decision.  

**The $p$-value of a test is the probability of getting a value at least as extreme as the observed value under the null hypothesis.** The $p$-value will always be between 0 and 1. A small $p$-value means the observed value is unlikely to occur if the null hypothesis were true. A large $p$-value means the observed value is likely to occur if the null hypothesis were true.  

The $p$-value of the test is printed in the third line. In this case it reads `p-value < 2.2e-16`. This is very small indeed. It means there is a near zero probability of observing a 14\% difference in wages if in reality the true difference in wages is zero.  

In other words, this test gives evidence to reject the null hypothesis. You may thus choose to conclude that the average difference in hourly wages between females and males is not zero, based on the evidence in this sample.  



## Interpreting p-values 

It is conventional to reject the null hypothesis if the $p$-value of a test is smaller than 0.05. However this is an arbitrary (and disputable) cutoff point, and you should use your own intuition to determine whether rejecting the null is a sensible choice, given the context.  

You can, for instance, use confidence intervals to determine whether rejecting the null is sensible. Note how the output of the $t$-test also gives you a 95\% confidence interval for the true mean, based on the sample data. In the example above, it suggests the true mean difference in wages is between 13.93\% and 14.47\%. Since this range is still substantially above zero, it supports your decision to reject the null.  



## Two-sample t-test for a difference in means

```{r, echo=FALSE}
# use 2019 dataset for this section
# because the 2018 data has too many observations
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
```

A two-sample $t$-test compares the means of two samples of data to see if they are really different. The null and alternative hypotheses are as follows:

- $H_0$: the true difference in the means is zero (i.e. both variables have the same true mean)
- $H_1$: the true difference in the means is zero not zero  (the variables have different means)

E.g. in the pay gap data, the variables `FemaleBonusPercent` and `MaleBonusPercent` record the average percentage bonus at each firm, for females and males respectively. You could construct a two-sample $t$-test on these variables, where the null hypothesis is that the true mean bonus percent is the same for both females and males:  

$$
\begin{aligned}
  H_0: \big| \;\;  \texttt{true female bonus percent - true male bonus percent} \;\; \big| = 0 \\ 
  H_1: \big| \;\; \texttt{true female bonus percent - true male bonus percent} \;\; \big| \neq 0
\end{aligned}
$$

In R you can use the `t.test()` function, giving the function both samples: 

```{r}
t.test(paygap$FemaleBonusPercent, paygap$MaleBonusPercent)
```

The observed values of the test are at the bottom: the average bonus percent is 25.50 for females and 26.03 for males, making the absolute difference between the two 0.5. Though nonzero, this difference is small.  

The $p$-value of this test is 0.889, i.e. there is an 88.9\% of seeing an observed difference of 0.5 under the null hypothesis. Since this $p$-value is substantially higher than 0.05, there is not sufficient evidence to reject the null. Moreover, the 95\% confidence interval *contains* the null hypothesis (that the difference is zero).  

You should thus assume the null is true, and conclude that there is no evidence in this sample to suggest the average bonus percent is different for females and males. 



\ 

# 4--4 Relationships between Variables 

The tools demonstrated thus far help understand the distribution of data within a *single* variable. 

Many studies are motivated by the relationships between two or more variables. This section will demonstrate a few tools for visualizing and quantifying relationships between variables. 



## Association and dependence

Two variables are said to be **associated** or **dependent** if there is a relationship between them. 

Scatterplots are a good way to visualize the association between two numeric variables. 

From the NYC heatwave data, below is a scatterplot of temperature on vegetation: 

```{r}
ggplot(aes(x = vegetation, y = temperature), data = nycheat) +
  geom_point(size=1) +
  xlab('vegetation (NDVI)') + ylab('temperature (farenheit)') +
  theme_bw()
```

There appears to be a negative association between temperature and vegetation, i.e. lower temperatures seem to be recorded in areas with a higher concentration of vegetation, and vice versa.  





## Correlation  

Correlation describes the *strength* of the association between two variables. Note that while association usually means any statistical relationship between two variables, correlation generally refers to linear associations only. 

The stronger the linear association between two variables, the greater their correlation. Strong correlation is depicted by tight clustering of data points, close to the diagonal line $y = x$ (or $y=-x$ for negative correlation). Weak correlation is depicted by loose clustering of data points, scattered away from the diagonal line.  

```{r fig-fullwidth, fig.width = 10, fig.height = 2, echo=FALSE}
knitr::include_graphics('./pics/m1c4_pic1.png')
```




## The Pearson correlation coefficient

There are many different measures of correlation. The most common for linear relationships is the Pearson correlation coefficient, denoted $r$, which takes a value between -1 and 1. A coefficient of $r=1$ or $r=-1$ implies perfect correlation, i.e. every data point is on the diagonal line. $r=0$ implies no correlation.  

```{r, echo=FALSE}
knitr::include_graphics('./pics/m1c4_pic2.png')
```

Note, the Pearson correlation coefficient is only responsive to linear relationships. In the above figure, the bottom row depicts variables that are clearly associated (indicated by the pattern) but since the association is nonlinear, the Pearson correlation coefficient is zero.  

In R you can use `cor()` to compute the Pearson correlation coefficient between two variables. E.g. the Pearson correlation coefficient between temperature and vegetation is:

```{r}
cor(nycheat$temperature, nycheat$vegetation)
```

```{r, echo=FALSE, fig.margin=TRUE, fig.cap="For reference: the relationship between temperature and vegetation. $r=-0.57$"}
ggplot(aes(x = vegetation, y = temperature), data = nycheat) +
  geom_point(size=1) +
  xlab('vegetation (NDVI)') + ylab('temperature (farenheit)') +
  theme_bw()
```

A correlation coefficient of $r=-0.57$ implies a moderate negative relationship between the two variables, as the scatterplot suggests.     



## Correlation does not imply causation 

Simply because two variables are correlated does not imply they are *causally* related. Demonstrating correlation is easy--but unless there is a clear causal mechanism between the two variables, it is much harder to prove that one actually causes the other. It is on you, the experimenter, to demonstrate a causal mechanism in your analysis.  

This webpage lists examples of **spurious correlations**--variables with demonstrable correlation but lacking any causal link. 

https://www.tylervigen.com/spurious-correlations 

\ 
