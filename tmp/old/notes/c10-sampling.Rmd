---
title: 'chapter 10 Samples, Sampling, and Variability'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
library(tufte)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# 10--1 Samples from Populations 

A quick recap of the terminology around samples and populations:   

A **population** is the complete set of observations required to conduct an experiment or answer a question fully---e.g. in the pay gap study, the target population might be the salary of _every_ person in the UK. It's often unrealistic (or impossible) to get data on a whole population, and samples are used instead.  

A **sample** is a subset of data from a population---e.g. the pay gap dataset is a *sample* of income data for 10,000 UK firms. Samples are used to represent populations, and make inferences about population characteristics which are often unknown. The **sample size** (number of observations) is typically denoted $n$.  

An **estimator** is a sample parameter that is used to estimate some unknown population parameter. There are point estimators and interval estimators---e.g. a sample mean is a *point estimator* for a population mean, and a confidence interval for a mean is an *interval estimator* for a population mean.  

A **random sample** is one where each observation is chosen randomly from the population. 

Below are the symbols typically used to denote sample and population parameters: 

```{r, echo=FALSE}
parameters = data.frame("parameter" = c("mean","standard deviation","correlation coefficient","regression coefficient"), "population" = c("$\\mu$","$\\sigma$","$\\rho$","$\\beta$"), "sample" = c("$\\bar x$","$s$","$r$","$b$"))
knitr::kable(parameters, caption = "Standard notation for population and sample parameters.")
```



\ 

# 10--2 Sample Variability 

Since every sample of data is unique, the estimates derived therefrom will vary randomly across different samples---a property known as **variability**. A truly random sample should be representative of the population, and give accurate estimates. But pure randomness is difficult to achieve, especially when sample size is small.  

To demonstrate this, the following code draws a random sample of 10 observations from a sample space comprising integers from 1 to 6 (like rolling a die 10 times):

`r margin_note("You can generate random samples in R with the $\\texttt{sample()}$ function. You must specify $\\texttt{x}$ (the sample space), $\\texttt{size}$ (the sample size), and $\\texttt{replace}$ (a logical indicating whether to sample with replacement).")`  

```{r, echo=FALSE}
set.seed(5)
```

```{r}
diceroll = sample(x = c(1,2,3,4,5,6), size = 10, replace = TRUE)
diceroll
```

If we have defined $X$ as the RV for the outcome of a single dice roll, the above output is a random sample of 10 observations of $X$. The sample mean, $\bar X$, is:  

```{r}
mean(diceroll)
```

If we repeat the process with another random sample, we will (likely) get a different set of observations, and a different sample mean:

```{r, echo=FALSE}
set.seed(12)
```

```{r}
diceroll2 = sample(x = c(1,2,3,4,5,6), size = 10, replace = TRUE)
diceroll2
mean(diceroll2)
```

Compare these sample means to the theoretical mean of the population, in this case $\mu = 3.5$. Clearly, the error^[Error is the difference between the sample value and the true of a parameter.] in each estimate varies substantially across the two observed samples.  

This a trivial example, since the RV in question follows a known distribution (discrete uniform), and we know the population mean exactly. In most experiments the population distribution is unknown---all we have is a sample distribution, and whatever estimates it produces.   

## Two sources of error in an estimator

There are two sources of error in sample-derived estimates:

- **sampling error**---the *random* tendency of an estimate to vary across different samples  
- **bias**---the *systematic* tendency of an estimate to over/underestimate the true population parameter  

The sampling error is a result of variability---the tendency of data to vary *randomly* across different samples. In the ensuing sections we introduce two convergence theorems---both crowning achivements in statistics---that describe how the variability of an estimate depends on sample size.  

Bias is the result of *systematic* (i.e. nonrandom) processes interfering with observations---e.g. poor calibration or selective sampling. In module 3 we will look at sources of bias---for now we will focus on concepts related to variability.  

## The reliability of an estimator 

Reliability refers to the _consistency_ of an estimator across different samples. The reliability of a result is closely related to its variability. An estimator with high variability---e.g. the sample mean in the "diceroll" simulation above---is considered *unreliable*, as its value tends to vary substantially across different samples.  



\ 

# 10--3 The Law(s) of Large Numbers

The law of large numbers is a theorem that describes how the variability of an estimate *decreases* with sample size.  

Here's a simple demonstration of this: the following code runs the "diceroll" simulation three times, with sample sizes $n = \{ 30, 1000, 10000 \}$. The sample mean is computed for each value of $n$.  

```{r, echo=FALSE}
set.seed(19)
```

```{r}
for (i in c(30,100,10000)) {
  diceroll = sample(x = c(1,2,3,4,5,6), size = i, replace = TRUE)
  cat(paste('sample mean when n =', i, ':', round(mean(diceroll),3),'\n'))
}
```

Predictably, increasing sample size brings the sample mean closer to its theoretical value, $\mu=3.5$. This is, in essence, the **law of large numbers**---it describes how the sample mean converges to its true value as the sample size becomes large.  

Formally, there are two versions of the law of large numbers:  

The **strong law of large numbers** says the sample mean converges *exactly* to the true mean for large $n$:

$$\bar X \rightarrow \mu \hspace{1cm} n \rightarrow \infty$$

The **weak law of large numbers** says the sample mean converges *in probability* to the true mean for large $n$. Specifically, it says the probability that the sampling error is larger than some nonzero constant goes to zero for large $n$:

$$\P \big( \big| \bar X - \mu \big| > c \big) \rightarrow 0 \hspace{1cm} n \rightarrow \infty$$
This may sound like a cumbersome definition---but all it really says is the sample mean $\bar X$ is *likely* to be near $\mu$ when $n$ is large. Crucially, the weak LLN leaves open the possibility that $\big| \bar X - \mu \big| > c$ at infrequent intervals, i.e. that *occasionally* the sample mean might deviate from the true mean, even with very large $n$. 

By contrast, the strong LLN describes an *exact* convergence to the true mean---it implies that $\big| \bar X - \mu \big| < c$ will be true in *all* cases when $n$ is large. This is a stronger convergence condition than the weak LLN, and there are many distributions for which the weak law holds, but the strong law does not.  



\ 

# 10--4 Sampling Distributions---a demonstration

In the "diceroll" simulation above we showed how repeating a random process with a new sample is bound to deliver different sample statistics. Below we show what happens when we do this repeatedly. In the following code, the same experiment (i.e. a die being rolled 30 times) is repeated *ten* times, each time with a new random sample. The 10 sample means are stored in a vector and printed:  

```{r}
N = 10 #number of trials
samplemeans = NULL #vector of sample means

for (i in 1:N) {
  diceroll = sample(x = c(1,2,3,4,5,6), size = 30, replace = TRUE)
  samplemeans[[i]] = mean(diceroll)
}

samplemeans
```

If $X$ is the RV for the outcome of a dice roll, the above output is not 10 observations of $X$, but a collection of sample means, $\bar X$, across 10 random samples, each of which has 30 observations of $X$. In other words, the above output is a **distribution of sample means**.  

Each sample mean is calculated $\bar X = \frac 1n \sum_i^n X_i$, where $X_i$ denotes a single observation in one of the samples.   

As it turns out, we can model the sample mean $\bar X$ as a random variable (provided we have a collection of sample means), which gives a **sampling distribution**. Below is a histogram of the 10 sample means computed above: 

```{r, fig.width=5, fig.height=3.5, fig.align='center'}
ggplot(data = as.data.frame(samplemeans), aes(x = samplemeans)) + 
  geom_histogram(binwidth = 0.1) + ggtitle(TeX('sampling distribution of $\\bar{X}$ (10 trials)')) + theme_bw()
```

This is known as the **sampling distribution of the mean**: it shows the distribution of the sample mean, $\bar X$, across several samples. Each sample in this case has size $n=30$ (i.e. 30 independent observations of $X$).   

As we repeat the experiment with more and more random samples, it turns out the shape of the sampling distribution converges. The plots below show the sampling distributions of $\bar X$ using 100, 1000, and 10,000 random samples respectively, each with $n=30$.   

```{r, echo=FALSE, fig.width=10, fig.height=3}
samplemeans=NULL

for (i in 1:11100) {
    dicetoss = sample(x = c(1,2,3,4,5,6), size = 30, replace = TRUE)
    samplemeans[i] = mean(dicetoss)
}

xbar100 = samplemeans[1:100]; xbar1000 = samplemeans[101:1100]; xbar10000 = samplemeans[1101:11000]

plot1 = ggplot(data = as.data.frame(xbar100), aes(x = xbar100)) +
  geom_histogram(binwidth = 0.1, aes(y = ..density..)) +
  ggtitle(TeX('sampling dist. of $\\bar{X}$ (100 trials)')) +
  xlab(TeX('$\\bar{X}$')) + 
  theme_bw()

plot2 = ggplot(data = as.data.frame(xbar1000), aes(x = xbar1000)) +
  geom_histogram(binwidth = 0.1, aes(y = ..density..)) +
  ggtitle(TeX('sampling dist. of $\\bar{X}$ (1000 trials)')) +
  xlab(TeX('$\\bar{X}$')) + 
  theme_bw()

plot3 = ggplot(data = as.data.frame(xbar10000), aes(x = xbar10000)) +
  geom_histogram(binwidth = 0.1, aes(y = ..density..)) +
  ggtitle(TeX('sampling dist. of $\\bar{X}$ (10,000 trials)')) +
  xlab(TeX('$\\bar{X}$')) + 
  theme_bw()

grid.arrange(plot1, plot2, plot3, ncol = 3)

xbar_summary = data.frame('number of trials' = c('100','1000','10,000'), 
                          mean = c(round(mean(xbar100),3), round(mean(xbar1000),3), round(mean(xbar10000),3)), 
                          'standard deviation' = c(round(sd(xbar100),3), round(sd(xbar1000),3), round(sd(xbar10000),3)))

#kable(xbar_summary, caption = 'Summary statistics for the three sampling distributions shown.')
```

Below are the summary statistics for each of these distributions:

```{r, echo=FALSE}
kable(xbar_summary)
```


\ 

There are three important things to take away from this demonstration:  

_**1 --- the sampling distribution of $\bar X$ approached a normal curve with a large number of trials**_

The convergence of a sampling distribution to a normal curve is the premise of the **central limit theorem**. Note how this convergence occurred even though the original RV was not normally distributed (the outcomes of a dice roll follow a discrete uniform distribution).  

This is an important result in statistics, as it implies the normal distribution can be used to model many random processes, even those that follow different distributions.  

_**2 --- the means of the sampling distributions tended to cluster around $\bar X \sim 3.5$, the theoretical mean of the distribution**_

The LLN predicts the convergence of the sample mean to the true mean, so this is an expected result.  

_**3 --- the standard deviations of the sampling distributions tended to cluster around $\SD[\bar X] \sim 0.3$**_

Note how this is much smaller than the s.d. of the population distribution.  

For reference, the s.d. of a discrete uniform distribution with values from 1 to $a$ is given by $\sigma = \sqrt{\frac{a^2-1}{12}}$. In this case, with values from 1 to 6, the population s.d. is $\sigma = 1.708$. The s.d. of the *sampling* distribution is clearly much smaller.  

It turns out the s.d. of a sampling distribution is scaled down from the population s.d. by a factor of $\sqrt n$:

$$\SD[\bar X] = \frac{\sigma}{\sqrt n} = \frac{1.708}{\sqrt{30}} = 0.31$$

This is another important result in statistics, and will be explained next. 



\ 

# 10--5 Sampling Distributions---some theory

Let's start from the beginning: if $X$ is a random variable for the outcome of a random process, then $X_i$ is often used to denote a single value/outcome/observation of this process. If you have a sample of $n$ observations of this random process, you have a collection of RVs $X_1, X_2, ..., X_n$.  

These RVs $X_1, X_2, ..., X_n$ are said to be **independent and identically distributed (i.i.d.)** if:

- each one is drawn from the same underlying population (which has some unknown but fixed distribution)
- the value of one outcome does not influence the values of other outcomes 

It's often assumed that observations in a sample are effectively i.i.d. This property leads to a useful result, as you will see.  

The sample mean, $\bar X$, is the mean of the observed outcomes in a random sample:  

$$\bar X = \frac 1n \sum_i^n X_i$$

where $X_i$ is a single observation in a random sample of size $n$.  

Different random samples will yield different values of $\bar X$. The distribution of $\bar X$ across different samples is called the **sampling distribution of the mean**.  

## The expected value of the sample mean, $\E[\bar X]$

Recall the following properties of expectation:

* the expected value of any one RV is simply the true mean of the population, i.e. $\E[X_i] = \mu$
* the expected value of each RV is the same (since they are i.i.d.)
* the expected value of a sum of RVs is equal to the sum of the expected values of the RVs, i.e. $\E \big[ \sum_i X_i \big] = \sum_i \E[X_i]$
* expectation scales linearly: $\E[aX] = a \E[X]$

It should follow intuitively that the expected value of a sample mean is simply the true mean: $\E[\bar X] = \mu$.   

To see this algebraically, apply the expectation operator to the formula for the sample mean, and do some wrangling: 

$$\E[\bar X] = \E \bigg[ \frac 1n \sum_i^n X_i \bigg] = \frac 1n \E \bigg[ \sum_i^n X_i \bigg] = \frac 1n \sum_i^n \E[X_i] = \frac 1n \sum_i^n \mu = \frac 1n \cdot n\mu = \mu$$
$$\therefore \;\; \E[\bar X] = \mu$$

We saw this result in the demonstration above: all three sampling distributions were centered around 3.5, the theoretical mean of the population distribution.    

## The variance of the sample mean, $\Var[\bar X]$

Recall the following properties of variance:   

* the variance of any one RV is simply the true variance of the population, i.e. $\Var[X_i] = \sigma^2$
* the variance of each RV is the same (since they are i.i.d.)
* the variance of a sum of independent RVs is equal to the sum of the variances of the RVs, i.e. $\Var \big[ \sum_i X_i \big] = \sum_i \Var[X_i]$
* variance scales by a square law: $\Var[aX] = a^2 \Var[X]$

The last property is what gives the variance of a sample mean its dependency on sample size: $\Var[\bar X] = \frac{\sigma^2}{n}$. 

To see this algebraically:  

$$\Var[\bar X] = \Var \bigg[ \frac 1n \sum_i^n X_i \bigg] = \frac{1}{n^2} \Var \bigg[ \sum_i^n X_i \bigg] = \frac{1}{n^2} \sum_i^n \Var[X_i] = \frac{1}{n^2} \sum_i^n \sigma^2 = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}$$
$$\therefore \;\; \Var[\bar X] = \frac{\sigma^2}{n}$$

The takeaway: _**the variance of a sample mean decreases as n increases**_.  

This is why, in the demonstration above, the sample s.d. was smaller than the population s.d.---it was scaled down by a factor of $\sqrt n$. Note $n$ always refers to the size of the *sample*, not the number of random samples used to create the sampling distribution. 

You can understand this phenomenon intuitively through the LLN, which shows that increasing $n$ makes the sample mean converge to the true mean. This is only possible if the data points are becoming more scattered around the true mean---which means the distribution must be getting taller and narrower as $n$ increases.    

To demonstrate this, the following code plots three sampling distributions, with sample sizes $n=\{ 10,50,100 \}$. The underlying RV is the same as above---a discrete uniform distribution with values from 1 to 6.  

```{r, fig.height=3.5, fig.width=5, fig.align='center'}
generator = function(n) {
  mean(rdunif(n, min = 1, max = 6))
}

sampledata = data.frame(Xbar = c(replicate(10000, generator(10)), 
                                 replicate(10000, generator(50)), 
                                 replicate(10000, generator(100))), 
                        n = c(rep(10,10000), rep(50,10000), rep(100,10000)) %>% as.factor())

ggplot(sampledata, aes(x = Xbar, fill = n)) + 
  geom_density(alpha = 0.3) + xlab(TeX('$\\bar{X}$')) + theme_bw()
```

Clearly, the dispersion of the sampling distribution decreases as $n$ increases.  

## The standard error, $\SE$

The standard deviation of the sample means is known as the **standard error:**

$$\SE = \SD[\bar X] = \sqrt{\Var[\bar X]} = \frac{\sigma}{\sqrt n}$$



\ 

# 10--6 Summary: Two Convergence Theorems in Statistics

The two convergence theorems can be summarized as follows:

The **law of large numbers** describes how the sample mean converges in *value* to its theoretical value for large $n$.  

The **central limit theorem** describes how the sample mean converges in *distribution* to a normal distribution for large $n$.   

These theorems concern the behavior of a *sequence* of random variables---this part of probability theory is known as **large sample theory** or **asymptotic theory**. These theorems are immensely useful for characterizing sample variability, and they have many applications in the statistical methods we introduced in module I.  

Below are two important caveats to these theorems you ought to be aware of:

_**When the LLN might fail:**_

It's possible the LLN may not hold if the random variables are themselves so dispersed that they don’t have an expected value. These are sometimes called “heavy-tailed” distributions. e.g. the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a>.  

_**Square-root convergence:**_

Note that when $n$ is initially small, increasing the sample size makes a significant change to the distribution, rapidly bringing the sample mean closer to the true mean, and rapidly reducing the variance. But as $n$ gets larger, it takes longer for the sample mean to get closer to the true value by the same amount---the **law of diminishing information**. 

You can see this in the expression for standard error: $\SD[\bar X] = \frac{\sigma}{\sqrt n}$. The dispersion of the sampling distribution does not decrease linearly, but over the *square root* of sample size. 

You can also see this in the plot above: the dispersion of the distribution reduces substantially when $n$ is increased from 10 to 50, but the change is more marginal when $n$ is increased from 50 to 100.  

\ 

---

\ 

\ 

