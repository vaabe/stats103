---
title: 'chapter 21 Model Selection \& The Bias-Variance Tradeoff'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
uscrime = read.csv("./data/uscrime.csv") %>%
  select(crime_rate = R,  age = Age, southern_states = S, edu = Ed, ex0  = Ex0, ex1 = Ex1, labor = LF, 
         number_of_males = M,  population = N,  unemployment_14_24 = U1, unemployment_35_39 = U2, wealth = W)
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# 21--1 Understanding the Bias-Variance Tradeoff

Suppose we have the following regression model predicting crime rate using these 10 predictors:^[Download the data <a href="./data/uscrime.csv" download>here</a>. For more information on the variables go <a href="https://www.openml.org/d/1089">here</a>.]

```{r}
reg1 = lm(crime_rate ~ age + southern_states + edu + ex0 + ex1 + labor + number_of_males + population + unemployment_14_24 + unemployment_35_39 + wealth, data = uscrime)
summary(reg1)
```

*The question:* which variables should we keep/eliminate to improve the model?   

*The problem:* as more predictors are added to a model, its bias decreases and its variability increases. Where possible, smaller models with fewer predictors are preferable to larger ones, as they are simpler and have lower variability.    

We want to avoid the two extremes:

- **underfitting** --- the model is too simple  (not enough covariates). This is a *biased* solution. 
- **overfitting** --- the model is too complex (too many covariates). This is a *high-variance* solution---small changes to the data will change the solution a lot.  

A good model achieves the right balance between bias and variance: 

```{r,echo=FALSE, fig.align='center', out.width=500, out.height=300, fig.cap="Source: http://scott.fortmann-roe.com/docs/BiasVariance.html"}
knitr::include_graphics("./pics/c21-pic1.png")
```

Thus finding a good model involves trading between fit and complexity.  



\ 

# 21--2 Criteria for Selecting Predictors

__*Adjusted $R^2$*__

We mentioned in chapter 16 that $\bar R^2$ is a measure of goodness-of-fit that incorporates a penalty on the number of predictors in the model. 

$$\bar R^2 = 1-\frac{\text{SSR}(p)}{TSS} - \frac{\text{SSR}(p)}{TSS} \cdot \frac{p}{N-p-1}$$

where $p$ is the number of predictors in the model. 

__*AIC*__

AIC (Akaike Information Criterion) is a measure of model quality---it can be thought of as goodness-of-fit minus  model complexity. AIC can deal with the risks of overfitting and underfitting.  

Formally:

$$\text{AIC}(p) = \ln \bigg( \frac{\text{SSR}(p)}{N} \bigg) + (p+1) \frac 2N$$

AIC rewards goodness-of-fit (the first term) but penalizes complexity (the second term, which is a penalty that increases with $p$).  

Among a set of models, the preferred one is the one that minimizes the AIC score. 

__*BIC*__

Another scoring method for assessing model quality is is BIC (Bayesian Information Criterion).  

Formally:


$$\text{BIC}(p) = \ln \bigg( \frac{\text{SSR}(p)}{N} \bigg) + (p+1) \frac{\ln  N}{N}$$

BIC also rewards goodness-of-fit and penalizes complexity (BIC penalizes complexity more harshly than AIC).  

Among a set of models, the preferred one is the one that minimizes the BIC score.  

In the next section we'll show you a simple way to conduct a model search using AIC or BIC scoring criteria.  




\ 

# 21--3 Forward and Backward Stepwise Regression 

Suppose we have $p$ predictors to choose among. Performing a model search involves searching through all $2^p$ possible models and selecting the one with the best score. If $p$ is relatively small, we can do a complete search over all the possible models.  

Two common methods are *forward stepwise regression* and *backward stepwise regression*.  

- **forward stepwise regression** --- start with no predictors, then add the variable that leads to the best score, and repeat the process---keep adding variables one by one until the score no longer improves
- **backward stepwise regression** --- start with the full model (all predictors), then remove the variable that leads to the best score, and repeat  

The following code applies backward stepwise regression to the crime data using AIC: 

```{r, warning=FALSE}
library(MASS)

# full model
reg2 = lm(crime_rate ~ ., data = uscrime)

# backward stepwise regression
reg3 = stepAIC(reg2, direction = "backward")

summary(reg3)
```


\ 

---

\ 

\ 

