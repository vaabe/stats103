---
title: 'extras'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
library(tufte)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---




## The BH method 



by arranging the $p$-values $p_1, ..., p_m$ in *increasing* order, then for a given $\alpha$, finding the largest $k$ such that $p_k \leq k \frac \alpha m$. We then reject all null hypotheses $H_{0i}$ for $i = 1,...,k$.  

Some definitions:

- the false discovery rate (FDR) is the number of false rejections divided by the number of rejections 
- the false discovery proportion (FDP) is the proportion of false rejections
- given $m$ tests, let $m_0$ be the number of null hypotheses that are *true*
- let $m_1$ be the number of null hypotheses that are false, such that $m_1 = m - m_0$

Construct the following table showing the outcomes:

```{r, echo=FALSE}
multtests = as.table(rbind(c('$U$','$V$','$m_0$'), c('$T$','$S$','$m_1$'), c('$m-R$','$R$','$m$')))
dimnames(multtests) = list(' ' = c('$H_0$ true','$H_0$ false','total'), 
                           ' ' = c('$H_0$ not rejected','$H_0$ rejected','total'))

kable(multtests)
```

Using this table, define the FDP as follows:

$$\text{FDP} = \begin{cases} V/R & R > 0 \\ 0 & R=0 \end{cases}$$

Then, if $p_1, ..., p_m$ are the **ordered** $p$-values from the $m$ tests, define:

$$k_i = \frac{i}{C_m} \frac \alpha m \hspace{1cm} R = \text{max} \big\{ p_i < k_i \big\}$$

where $C_m = 1$ if the $p$-values are independent, and if not, then $C_m = \sum_i^m \frac 1i$. If we let $T = p_R$, then $T$ is the **BH rejection threshold**. We can thus reject all null hypotheses $H_{0i}$ for which $P_i \leq T$.   




## Proof of minimum variance

In a simple regression the variance of the LS estimator for $\beta_1$ is given (from <a href="https://stats103.org/notes/c15-regression-theory.html#variance-of-the-ls-estimator">chapter 15</a>):

$$\Var[b_1] = \frac{1}{\sum_i(x_i - \bar x)^2}$$

We can define another linear estimator $\tilde b_1 = \sum_i c_i y_i$. If this estimator is unbiased it must follow that:

$$\E [\tilde b_1] = \sum_i c_i \E[y_i] = \sum_i c_i (\beta_0 + \beta_1 x_i)$$
$$= \beta_0 \sum_i c_i + \beta_1 \sum_i c_i x_i = \beta_1$$

For the above equation to be true, it must follow that $\sum_i c_i = 0$ and $\sum_i c_i x_i = 1$. 



\ 

\ 

---

\ 




