---
title: 'extras2'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
library(tufte)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

--- 

Here we run a simulation showing how multicollinearity in the data can affect the variability in the estimated regression coefficients.  

In the code below, the function `collinear_datagen()` generates a data for a regression model with the equation:

$$y = x_1 +  2x_2 + 3x_3$$

where the variable $x_3$ is linearly dependent on $x_1$ and $x_2$ (more so with the $x_1$ than $x_2$---see the code). The constant `r` determines the extent of the multicollinearity---the smaller it is, the higher the multicollinearity between the variables.  

The function `sim` runs the simulation `N` times and saves the regression coefficients, their standard errors, and their $p$-values. Below are histograms showing the variability in the estimated coefficients $b_1$, $b_2$, and $b_3$, under two values of $r$: 0.1 (lower multicollinearity) and 0.01 (higher multicollinearity).  

```{r, fig.width=11, fig.height=5, fig.fullwidth=TRUE}
## generate collinear data
collinear_datagen = function(n,r) {
  
  x1 = rnorm(n)
  x2 = rnorm(n)
  x3 = 0.8*x1 + 0.2*x2 + sqrt(r)*rnorm(n)
  y = x1 - 2*x2 + 3*x3 + rnorm(n)
  
  sample = data.frame(y,x1,x2,x3)
  
  model = summary(lm(y ~ x1 + x2 + x3, data = sample))
  modeldata = as.list(data.frame(model$coefficients[-1,c(1,2,4)]))
}

## run simulation
sim = function(N,n,r) {
  
  simulation = replicate(N, collinear_datagen(n,r))
  betas = do.call(rbind, simulation[1,])
  SEs =  do.call(rbind, simulation[2,])
  pvals =  do.call(rbind, simulation[3,])
  results = data.frame(cbind(betas, SEs, pvals))
  names(results) = c("b_1", "b_2", "b_3", "SE_1", "SE_2", "SE_3", "pval_1", "pval_2", "pval_3")
  results
  
}

## save simulation data
mc1 = sim(500,100,0.1) #coefficient simulation with lower multicollinearity
mc01 = sim(500,100,0.01) #coefficient simulation with higher multicollinearity 
mc1$r = "0.1"
mc01$r = "0.01"

Rsq1 = sim(500,100,0.1)
Rsq01 = sim(500,100,0.01)
Rsq1$r = "0.1"
Rsq01$r = "0.01"

mc = rbind(mc1,mc01)

## plots
plot1 = ggplot(mc, aes(b_1)) + geom_histogram(bins=30) + xlab(TeX("$b_1$")) + facet_grid(r~.) + geom_vline(xintercept = 1) + theme_light()
plot2 = ggplot(mc, aes(b_2)) + geom_histogram(bins=30) + xlab(TeX("$b_2$")) + facet_grid(r~.) + geom_vline(xintercept = -2) + theme_light()
plot3 = ggplot(mc, aes(b_3)) + geom_histogram(bins=30) + xlab(TeX("$b_3$")) + facet_grid(r~.) + geom_vline(xintercept = 3) + theme_light()

grid.arrange(plot1, plot2, plot3, ncol=3)
```

The vertical lines on the plots show the population coefficients---which are $\beta_1 = 1$, $\beta_2 = -2$, and $\beta_3 = 3$. It's clear from these plots that the data with higher multicollinearity (with r = 0.01) has higher variability in its coefficient estimates.  

Below is a correlation matrix that summarizes the pairwise correlations between the variables $x_1$, $x_2$, and $x_3$. Notice how $x_1$ and $x_3$ are strongly correlated with each other: 

```{r}
kable(cor(mc[ ,1:3]))
```

To see how this affects power, below is a table summarizing the power of each cofficient under $r=0.01$ and $r=0.1$:

```{r}
# see how this is reflected in the power
power = mc %>% group_by(r) %>% summarize(power_b1 = mean(pval_1 < 0.05), power_b2 = mean(pval_2 < 0.05), power_b3 = mean(pval_3 < 0.05))
kable(power)
```





