---
title: "Validity and Bias"
chapter: "17"
part: "pt4"
output:
  html_document:
    css: "tufteish.css"
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(extraDistr)
library(gridExtra)
library(latex2exp)
set.seed(5)
ggtheme = theme_light() +
	theme(panel.background = element_rect(fill = '#fffff8')) +
	theme(plot.background = element_rect(fill = '#fffff8'))
ggplot = function(...) ggplot2::ggplot(...) + ggtheme
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

# Two Sources of Error in an Estimator 

To recap: in chapter 10 we mentioned that there two sources of error in estimation problems:

- **variability** or **random sampling error**---the *random* tendency of a result to vary across different samples 
- **bias**---the *systematic* tendency of a result to over/underestimate its true value 

Both can contribute to the overall error in an estimate. In module 2 we focused on variability, and we introduced two convergence theorems that describe how variability decreases with sample size.  

Bias is a more insidious source of error---its presence indicates there is something wrong with the sampling/measurement technique, or with the design of the experiment. Bias is systematic (not due to random fluctuations) and cannot be reduced by increasing the sample size.  

Formally, the bias of an estimator $\hat\theta$ is the difference between its expected value and the underlying population value $\theta$ being estimated: 

$$\Bias[\hat\theta] = \E[\hat\theta] - \theta$$

i.e. there is bias when the *expected value* of the estimator differs from the true value. 

## Reliability vs. Validity 

Two important concepts associated with variability and bias:

- **reliability**---the *consistency* of a result, i.e. the extent to which it produces similar values in different samples
- **validity**---the *accuracy* of a result, i.e. the extent to which it reflects what is actually trying to be measured

Reliability is related to variability---a result is *reliable* if it has low variability, and vice versa. Validity is related to bias---a result is *valid* if it has high accuracy, and vice versa. 

Note that reliability does not imply validity---e.g. a scale can produce reliable (consistent) estimates of mass, but if incorrectly calibrated, its estimates will be *systematically* incorrect.  

There are two major kinds of validity in experiments: *internal* validity and *external* validity.    

## External validity

External validity concerns the *generalizability* of a result, i.e. whether it will hold in different samples/settings. For a result to have external validity, it should be *portable*, and it should not depend on in-sample conditions.  

A common threat to external validity is a non-representative sample---this is known as **sampling bias.** Results from a biased sample will only be valid for that sample, and cannot be generalized to the entire  population.      

## Internal validity  

Internal validity concerns the correct identification of causal relationships in an experiment. For a result to have internal validity, it must be controlled for by confounding variables, and it must be properly measured/identified. Some common threats to internal validity: 

- confounding variables/events that are not controlled for---**omitted variable bias**
- incorrectly identified variables and causal relationships---**specification bias**
- poorly made/calibrated measurements---**measurement error bias**  

In the remainder of this module we will examine sources of bias.   

# Estimator Bias

An estimator is biased if its expected value differs from the true value of the parameter being estimated.  

If $\hat\theta$ is an estimator for the true parameter $\theta$, the bias of the estimator can be written:

$$\Bias[\hat\theta] = \E[\hat\theta] - \theta$$

## Unbiased estimators---the sample mean

From asymptotic theory we know that the expected value of the sample mean converges to its true value when $n$ is large: $\E[\bar X] \longrightarrow \mu$. The bias of the sample mean is thus:

$$\Bias[\bar X] = \E[\bar X] - \mu = \mu - \mu = 0$$

In other words, the sample mean is an *unbiased estimator* for the true mean. If you want to estimate the tue mean of a population, the sample mean should give you an accurate estimate (provided there are no other sources of bias in the sample).    

Not all estimators are inherently unbiased, as the next section will demonstrate.  

## Biased estimators---the sample maximum

Suppose now we want to estimate the *maximum* value of a distribution. Is the sample maximum an unbiased estimator for the true maximum? As it turns out, the answer is no. 

To demonstrate this, let $X$ be a continuously distributed RV between 0 and 10, i.e. $X \sim \mathcal U(0,10)$. The following code generates 100 random observations of $X$ and plots the histogram:

```{r, echo=FALSE}
set.seed(26)
```

```{r, fig.height=3.5, fig.width=5, fig.align='center'}
X = runif(n = 100, min = 0, max = 10)

ggplot(aes(x = X), data = as.data.frame(X)) + 
  geom_histogram(binwidth = 0.5) 
```

If $\theta$ denotes the true maximum of the population distribution, we know that in this case $\theta = 10$. The sample maximum, $\hat\theta$, is:

```{r}
max(X)
```

If we repeat the process many times, we can construct a sampling distribution of the maximum:

```{r, fig.height=3.5, fig.width=5, fig.align='center'}
thetahat = replicate(n = 1000, max(runif(n = 100, min = 1, max = 10)))

ggplot(aes(x = thetahat), data = as.data.frame(thetahat)) + 
  geom_histogram(bins = 50) + ggtitle('sampling distribution of the maximum') + xlab(TeX('$\\hat{\\theta}$')) 
```

Note how the sampling distribution of $\hat\theta$ is neither bell-shaped nor centered at the true maximum $\theta$. In fact, based on this sampling distribution, the expected value of the sample maximum is:

```{r}
mean(thetahat)
```

Clearly, the sample maximum underestimates the true maximum, since $\E[\hat\theta] - \theta =$ `r toString(round(mean(thetahat),3))` -10 $< 0$. The bias of the sample maximum in this example is:

```{r}
mean(thetahat) - 10
```

With a bit of calculus you can show that, in fact, the expected value of the sample maximum is

$$\E[\hat\theta] = \frac{n}{n+1} \theta$$

i.e. the sample maximum is a *biased* estimator for the true maximum, since it will consistently underestimate the true maximum by a factor $\frac{n}{n+1}$. See the proof <a href="https://en.wikipedia.org/wiki/German_tank_problem">here</a>.  

Using this fact we can construct a bias-corrected estimator for the true maximum:

$$\frac{n+1}{n} \hat\theta$$
where $\hat\theta$ is the sample maximum. 

For the above example, where $n=100$, the bias-corrected estimate of the true maximum is:

```{r}
mean(thetahat)*((100+1)/100)
```

which is clearly much more *accurate* than the uncorrected sample value. 

## Bessel's Correction

Another example of estimator bias arises when estimating the variance of a distribution. Formally, the population variance is defined $\Var[X] = \E[(X-\mu)^2]$, or written as a sum:

$$\sigma^2 = \frac 1n \sum_i^n (X_i - \mu)^2$$

It turns out this formula is only valid for population data. When using sample data, it will consistently *underestimate* the population variance by a factor $\frac{n-1}{n}$:

$$\E \bigg[ \frac 1n \sum_i^n (X_i - \bar X)^2 \bigg] = \frac{n-1}{n} \sigma^2$$

See the proof <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction#Formula">here</a>. This bias can be remedied by using $n-1$ in the denominator instead of $n$. The bias-corrected formula for population variance is:

$$s^2 = \frac{1}{n-1} \sum_i^n (X_i - \bar X)^2$$

which is known as **Bessel's correction.**. 

Note that when $n$ is large, the difference between $\frac 1n$ and $\frac{1}{n-1}$ becomes negligible---the difference between the two formulae is significant only for small samples. This is why we use the $t$-distribution for small samples, since it uses the bias-corrected formula for variance, which produces a normal curve with fatter tails. This is also why the $t$-distribution only has one parameter, DoF, since each $n$ produces a slightly different curve.  

# Sampling Bias 

To recap: a **random sample** is one where each observation is selected randomly from the population, and has an equal probability of being selected.  

**Sampling bias** occurs when the observations in a sample are selected in a non-random way (certain observations in the population have a higher/lower probability of being selected than others). It results in a non-representative sample.  

## Types of sampling bias 

Below are some common types of sampling bias---

__Self-Selection Bias__---can occur if study participants self-select (i.e. have control over whether to participate). It could be that the people who voluntarily opt in tend to represent a certain group in the population (e.g. with particularly strong opinions/characteristics), resulting in a biased sample. 

- e.g. using responses from polls or voluntary surveys---respondents may tend to have stronger opinions than nonrespondents, resulting in an overrepresentation of "extreme" opinions.
- also see---<a href="https://en.wikipedia.org/wiki/Participation_bias">participation bias</a>. 

__Referral bias__ *(aka Berkeson's fallacy)*---can occur if the study population is selected from a certain environment that differs from the general population/control group.  

- e.g. in hospital studies, if the admissions rates to the hospital are different for certain ailments (e.g. admission rates of exposed cases and controls differ), the association between exposure and ailment can be distorted. This can result in spurious negative correlations between ailments. <a href="https://wiki.ecdc.europa.eu/fem/w/wiki/referral-bias">Example</a>.
- also see---<a href="https://catalogofbias.org/biases/admission-rate-bias/">admission rate bias</a>.
    
__Survivorship Bias__---can occur if the study only focuses on observations that "survived" after some selection criteria. Ignoring/excluding "failures" can result in optimistic beliefs about the characteristics associated with "successes". 

- e.g. WWII planes---during the war the US military thought it could reduce aircraft casualties by adding extra armour to its fighter planes where the returning planes showed most damage. But in doing so it only considered planes that *survived*---the planes that were shot down were excluded from the damage analysis. This is a classic example---read more <a href="https://mcdreeamiemusings.com/blog/2019/4/1/survivorship-bias-how-lessons-from-world-war-two-affect-clinical-research-today">here</a>.  

__Response Bias__---the tendency for participants to give misleading responses due to behavioral/environmental inputs. Read more <a href="https://en.wikipedia.org/wiki/Response_bias">here</a>.

## Sampling methods 

Below are some common methods for sampling from a population---

__Simple Random Sampling__---a method whereby each observation in the population has an equal probability of being selected. If carried out properly, this method should minimize bias and ensure a fairly representative sample. An issue with SRS is its susceptibility to random sampling error, which may accidentally result in a biased sample---e.g. if a population has 50\% woman and 50\% men, random sampling will on *average* produce representative proportions of each gender, but in any one sample the proportions may be slightly off due to variability. The smaller the sample, the more susceptible it is to this problem.   

__Stratified Sampling__---a method whereby the population is divided into categories or "strata", and each stratum is then sampled individually using a second sampling method (usually SRS). This method is useful if the population comprises distinct groups (e.g. people of different races) and it's of particular importance that each group is fairly represented (e.g. if the group is correlated with some effect/response). Stratified sampling will ensure each group is represented in the sample, where simple random sampling may over/underrepresent certain groups due to chance variability. Note that stratified sampling is only advantageous over SRS if the population can be divided into *distinct* groups that are relatively homogeneous (i.e. the groups should have lower variability than the population as a whole). If so, stratified sampling will produce smaller errors in estimation. Check out <a href="https://newonlinecourses.science.psu.edu/stat506/node/27/">this link</a> for more on errors and CIs in stratified sampling.   

## Correcting for sample bias

If *entire* groups in a population are excluded from a sample, there is no way to correct the bias in subsequent estimates. However if the sample *underrepresents* certain groups, and the true population proportions are known (or can be guessed), then the bias can be corrected by **weighting** each group.  

E.g. if a population is known to have 50\% females and 50\% males, but a sample has 60 females and 40 males, the bias could be corrected by weighting each female observation by $\frac{50}{60}=0.833$ and each male observation by $\frac{50}{40}=1.25$. This correction would make subsequent estimates have the same expected value as they would in a representative sample. Note that weighted sampling requires prior knowledge of the population proportions, and it doesn't account for the possibility that females and males might have differed in their *likelihood* of being selected (this could be a problem in self-selected or environment-dependent samples).  

# Some Consequences of Bias  

If $X$ is a RV for the observed outcomes in a random unbiased sample, then we should expect that $\E[X] = \mu$,  and that the larger the sample, the better the approximation (the LLN).  

But if the sampling method is biased, and certain segments of the population are excluded/underrepresented, the expected value of $X$ is no longer the population mean, but rather the mean of the biased subset, i.e. $\E[X] = c$ where $c \neq \mu$. In this case increasing $n$ will make $\bar X$ converge to $c$, not $\mu$.  

## The LLN and CLT

In general, paramater estimates will converge to expected value of the (sub)population represented in the sample.  

The general statement of the law of large numbers:

$$\bar X \longrightarrow \E[X]$$

i.e. when $n$ is large, the sample mean will converge to its *expected value*. Convergence to the true mean $\mu$ requires that $\E[X]=\mu$, which is only possible if the sampling method is unbiased and the sample is representative.  

Similarly, the CLT can be stated generally:

$$\bar X \sim \mathcal N \bigg( \E[X], \frac{\Var[X]}{n} \bigg)$$

## Effects on intervals   

Since confidence intervals are centered on the sample mean, $\bar X$, the probability that a given interval contains the true mean depends on how close $\bar X$ is to $\mu$. If the sample is biased, confidence intervals will be centered somewhere other than $\mu$, and the probability of capturing the true mean will converge to zero as $n$ increases.  

E.g. suppose a population has true mean $\mu = 10$. If our sample is biased, such that $\E[X] = 12 \neq \mu$, a 95\% confidence interval computed on $\bar X$ will be centered at 12, not 10. With $s = 6$ and $n=20$, a 95\% confidence interval will asymptotically cover the following region:

```{r, warning=FALSE, echo=FALSE, fig.width = 5, fig.height=3.5, fig.align='center'}
Xbar = 12; s = 5; n = 20

SE = s/sqrt(n)

x = seq(-4, 4, length = 1000) * SE + Xbar
y = dnorm(x, Xbar, SE)

lb = Xbar + qt(0.05/2, df = n-1) * SE
ub = Xbar + qt(1-0.05/2, df = n-1) * SE

df = data.frame(x = x, y = y)

breaks = round(seq(Xbar-4*SE, Xbar+4*SE, SE),2)

ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(limits = c(7,17), breaks = c(7:17))  + 
  scale_y_continuous(limits = c(0,0.4)) +
  xlab(TeX("$\\bar{X}$")) +
  ylab('probability') +
  ggtitle(TeX('95% confidence interval under bias with $n=20$')) +
  geom_area(aes(x = ifelse(x > lb & x < ub, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = lb, xend = lb, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') + 
  geom_segment(x = ub, xend = ub, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = 10, xend = 10, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$\\bar{X}$'), x = 12, y = 0.2, size = 6, color = 'violetred') +
  geom_text(label = TeX('$-2\\frac{s}{\\sqrt{n}}$'), x = lb, y = 0.12, size = 4, color = 'violetred') +
  geom_text(label = TeX('$+2\\frac{s}{\\sqrt{n}}$'), x = ub, y = 0.12, size = 4, color = 'violetred') +
  geom_text(label = TeX('$\\mu$'), x = 10, y = 0.2, size = 6, color = 'black') 
  
```

i.e. with $n=20$ it contains $\mu$, but only just. 

If the sample had $n=60$, the 95\% confidence interval no longer contains $\mu$:

```{r, warning=FALSE, echo=FALSE, fig.width = 5, fig.height=3.5, fig.align='center'}
Xbar = 12; s = 5; n = 60

SE = s/sqrt(n)

x = seq(-4, 4, length = 1000) * SE + Xbar
y = dnorm(x, Xbar, SE)

lb = Xbar + qt(0.05/2, df = n-1) * SE
ub = Xbar + qt(1-0.05/2, df = n-1) * SE

df = data.frame(x = x, y = y)

breaks = round(seq(Xbar-4*SE, Xbar+4*SE, SE),2)


ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(limits = c(7,17), breaks = c(7:17))  + 
  scale_y_continuous(limits = c(0,0.7)) +
  xlab(TeX("$\\bar{X}$")) +
  ylab('probability') +
  ggtitle(TeX('95% confidence interval under bias with $n=60$')) +
  geom_area(aes(x = ifelse(x > lb & x < ub, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = lb, xend = lb, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') + 
  geom_segment(x = ub, xend = ub, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.9, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = 10, xend = 10, y = 0, yend = 0.9, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$\\bar{X}$'), x = 12, y = 0.3, size = 6, color = 'violetred') +
  geom_text(label = TeX('$-2\\frac{s}{\\sqrt{n}}$'), x = lb, y = 0.12, size = 4, color = 'violetred') +
  geom_text(label = TeX('$+2\\frac{s}{\\sqrt{n}}$'), x = ub, y = 0.12, size = 4, color = 'violetred') +
  geom_text(label = TeX('$\\mu$'), x = 10, y = 0.3, size = 6, color = 'black') 
  
```


If the sample had $n=200$:

```{r, warning=FALSE, echo=FALSE, fig.width = 5, fig.height=3.5, fig.align='center'}
Xbar = 12; s = 5; n = 200

SE = s/sqrt(n)

x = seq(-4, 4, length = 1000) * SE + Xbar
y = dnorm(x, Xbar, SE)

lb = Xbar + qt(0.05/2, df = n-1) * SE
ub = Xbar + qt(1-0.05/2, df = n-1) * SE

df = data.frame(x = x, y = y)

breaks = round(seq(Xbar-4*SE, Xbar+4*SE, SE),2)

ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(limits = c(7,17), breaks = c(7:17))  + 
  scale_y_continuous(limits = c(0,1.2)) +
  xlab(TeX("$\\bar{X}$")) +
  ylab('probability') +
  ggtitle(TeX('95% confidence interval under bias with $n=100$')) +
  geom_area(aes(x = ifelse(x > lb & x < ub, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = lb, xend = lb, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') + 
  geom_segment(x = ub, xend = ub, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 2, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = 10, xend = 10, y = 0, yend = 2, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$\\bar{X}$'), x = 12, y = 0.5, size = 6, color = 'violetred') +
  geom_text(label = TeX('$-2\\frac{s}{\\sqrt{n}}$'), x = lb, y = 0.12, size = 4, color = 'violetred') +
  geom_text(label = TeX('$+2\\frac{s}{\\sqrt{n}}$'), x = ub, y = 0.12, size = 4, color = 'violetred') +
  geom_text(label = TeX('$\\mu$'), x = 10, y = 0.5, size = 6, color = 'black') 
  
```


The takeaway---while increasing $n$ reduces the variability of an estimate, if the sample is biased it will also decrease the probability that the interval contains the true value. As $n$ gets large the "coverage" probability of the interval will converge to zero---eventually none of the intervals will contain the true parameter, even if some initially did.  

The theme of trading between bias and variance is an important one in statistics---often it's not possible to minimize both simultaneously.  

## Effects on power 

Recall the **power** of a test is the probability of correctly rejecting a false null. 

E.g. if a null hypothesis has $H_0: \mu = 12$ but the true mean is $\mu = 9$, the power of the test can be visualized as the following region:  

```{r, warning=FALSE, echo=FALSE, fig.width = 5, fig.height=3.5, fig.align='center'}
Xbar = 12   #sample mean
s = 6   #sample s.d.
n = 20   #sample size
SE = s / sqrt(n)   #standard error
mu = 9

x = seq(-10, 4, length = 1000) * SE + Xbar
y = dnorm(x, Xbar, SE)
y1 = dnorm(x, mu, SE)

df = data.frame(x = x, y = y)
df1 = data.frame(x = x, y = y1)

breaks = round(seq(Xbar-6*SE, Xbar+4*SE, SE),2)

alpha5 = 0.05
alpha1 = 0.01

Z_5 = qnorm(1-alpha5/2)
Z_1 = qnorm(1-alpha1/2)

ggplot(data = df1, mapping = aes(x = x, y = y)) +
  geom_line(color = 'violetred') +
  geom_line(data = df, aes(x = x, y = y), color = 'black') +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-6*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(' ') +
  ggtitle(TeX('power if $\\mu = 9$ ($H_0$ false)')) +
  geom_area(aes(x = ifelse(x > Xbar - Z_5*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='blue', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') +
  geom_text(label = 'Type II Error', x = 11, y = -0.005, size = 3, color = 'violetred') +
  geom_text(label = 'Power', x = 7.1, y = -0.005, size = 3, color = 'blue') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu_{H_0}$'), x = Xbar, y = 0.2, size = 5, color = 'black') +
  geom_segment(x = mu, xend = mu, y = 0, yend = 0.5, color = 'violetred', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu$'), x = mu, y = 0.2, size = 5, color = 'violetred') 
  #geom_text(label = TeX('$\\bar{X}$'), x = Xbar, y = 0.2, size = 5, color = 'black') +
  
```


But if the sample is biased, such that $\E[X] = 10.5$, the power of the test is now:

```{r, warning=FALSE, echo=FALSE, fig.width = 5, fig.height=3.5, fig.align='center', fig.cap="The green line represents the distribution of the biased sample."}
Xbar = 12   #sample mean
s = 6   #sample s.d.
n = 20   #sample size
SE = s / sqrt(n)   #standard error
mu = 10

Xbar_bias = 10.5

x = seq(-10, 4, length = 1000) * SE + Xbar
y = dnorm(x, Xbar, SE)
y1 = dnorm(x, mu, SE)
y2 = dnorm(x,Xbar_bias, SE)

df = data.frame(x = x, y = y)
df1 = data.frame(x = x, y = y1)
df2 = data.frame(x = x, y = y2)

breaks = round(seq(Xbar-6*SE, Xbar+4*SE, SE),2)

alpha5 = 0.05
alpha1 = 0.01

Z_5 = qnorm(1-alpha5/2)
Z_1 = qnorm(1-alpha1/2)

ggplot(data = df2, mapping = aes(x = x, y = y)) +
  geom_line(color = 'darkcyan') +
  geom_line(data = df, aes(x = x, y = y), color = 'black') +
  geom_line(data = df1, aes(x = x, y = y), color = 'violetred') +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-6*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(' ') +
  ggtitle(TeX('power if $\\mu = 9$ ($H_0$ false) under bias')) +
  geom_area(aes(x = ifelse(x > Xbar - Z_5*SE, x, 0)), fill='cyan', alpha=0.4) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='blue', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') +
  geom_text(label = 'Type II Error', x = 11, y = -0.005, size = 3, color = 'darkcyan') +
  geom_text(label = 'Power', x = 7.1, y = -0.005, size = 3, color = 'blue') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu_{H_0}$'), x = Xbar, y = 0.2, size = 5, color = 'black') +
  geom_segment(x = mu, xend = mu, y = 0, yend = 0.5, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = Xbar_bias, xend = Xbar_bias, y = 0, yend = 0.5, color = 'darkcyan', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu$'), x = mu, y = 0.2, size = 5, color = 'violetred') +
  geom_text(label = TeX('$\\bar{X}$'), x = Xbar_bias, y = 0.2, size = 5, color = 'darkcyan') 
  
```

Here the bias brings the sample mean closer to the null, which *reduces* the power of the test.   

Conversely, if the sample bias is such that $\E[X] = 7.5$, the power of the test is:

```{r, warning=FALSE, echo=FALSE, fig.width = 5, fig.height=3.5, fig.align='center'}
Xbar = 12   #sample mean
s = 6   #sample s.d.
n = 20   #sample size
SE = s / sqrt(n)   #standard error
mu = 10

Xbar_bias = 7.5

x = seq(-10, 4, length = 1000) * SE + Xbar
y = dnorm(x, Xbar, SE)
y1 = dnorm(x, mu, SE)
y2 = dnorm(x,Xbar_bias, SE)

df = data.frame(x = x, y = y)
df1 = data.frame(x = x, y = y1)
df2 = data.frame(x = x, y = y2)

breaks = round(seq(Xbar-8*SE, Xbar+4*SE, SE),2)

alpha5 = 0.05
alpha1 = 0.01

Z_5 = qnorm(1-alpha5/2)
Z_1 = qnorm(1-alpha1/2)

ggplot(data = df2, mapping = aes(x = x, y = y)) +
  geom_line(color = 'darkcyan') +
  geom_line(data = df, aes(x = x, y = y), color = 'black') +
  geom_line(data = df1, aes(x = x, y = y), color = 'violetred') +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-8*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(' ') +
  ggtitle(TeX('power if $\\mu = 9$ ($H_0$ false) under bias')) +
  geom_area(aes(x = ifelse(x > Xbar - Z_5*SE, x, 0)), fill='cyan', alpha=0.4) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='blue', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') +
  geom_text(label = 'Type II Error', x = 11, y = -0.005, size = 3, color = 'darkcyan') +
  geom_text(label = 'Power', x = 7.1, y = -0.005, size = 3, color = 'blue') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu_{H_0}'), x = Xbar, y = 0.2, size = 5, color = 'black') +
  geom_segment(x = mu, xend = mu, y = 0, yend = 0.5, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = Xbar_bias, xend = Xbar_bias, y = 0, yend = 0.5, color = 'darkcyan', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu$'), x = mu, y = 0.2, size = 5, color = 'violetred') +
  geom_text(label = TeX('$\\bar{X}$'), x = Xbar_bias, y = 0.2, size = 5, color = 'darkcyan') 
  
```

Here the bias brings the sample mean further away from the null, which *increases* the power of the test.   

The takeaway---a biased sample can increase or decrease the power of the test, depending on whether the bias brings the sample mean closer/further from the true mean.   

## Effects on Type I Error

If the null hypothesis is true, a biased sample can make us lose control of the type I error.  

E.g. if a null hypothesis has $H_0: \mu = 12$, and if the null is true, then under an *unbiased* sample the type I error is:

```{r, warning=FALSE, echo=FALSE, fig.width = 5, fig.height=3.5, fig.align='center'}
Xbar = 12
s = 6   #sample s.d.
n = 20   #sample size
SE = s / sqrt(n)   #standard error
mu = 10

x = seq(-10, 10, length = 1000) * SE + Xbar
y = dnorm(x, Xbar, SE)

df = data.frame(x = x, y = y)

ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-4*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(' ') +
  ggtitle(TeX('Type I Error if $\\mu = 12$ ($H_0$ true)')) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='black', alpha=0.4) +
  geom_area(aes(x = ifelse(x > Xbar + Z_5*SE, x, 0)), fill='black', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu_{H_0}$'), x = Xbar, y = 0.15, size = 2.5, color = 'black') +
  geom_text(label = 'Type I Error', x = 8.8, y = -0.005, size = 2.5, color = 'black') +
  geom_text(label = 'Type I Error', x = 15.7, y = -0.005, size = 2.5, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'black') 
  
```

But if the sample is biased, such that $\E[X] = 14$, the type I error is:

```{r, warning=FALSE, echo=FALSE, fig.width = 5, fig.height=3.5, fig.align='center'}
Xbar_bias = 14

y1 = dnorm(x, Xbar_bias, SE)

df1 = data.frame(x = x, y = y1)

ggplot(data = df1, mapping = aes(x = x, y = y)) +
  geom_line(color = 'darkcyan') +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-4*SE, Xbar+5*SE)) +
  geom_line(data = df, aes(x = x, y = y)) +
  ylab('probability') + xlab(' ') +
  ggtitle(TeX('Type I Error if $\\mu = 12$ ($H_0$ true) under bias')) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='black', alpha=0.4) +
  geom_area(aes(x = ifelse(x > Xbar + Z_5*SE, x, 0)), fill='black', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_segment(x = Xbar_bias, xend = Xbar_bias, y = 0, yend = 0.5, color = 'darkcyan', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu_{H_0}$'), x = Xbar, y = 0.2, size = 5, color = 'black') +
  geom_text(label = TeX('$\\bar{X}$'), x = Xbar_bias, y = 0.2, size = 5, color = 'darkcyan') +
  geom_text(label = 'Type I Error', x = 8.8, y = -0.005, size = 2.5, color = 'black') +
  geom_text(label = 'Type I Error', x = 15.7, y = -0.005, size = 2.5, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'black') 
  
```

If the test is two-tailed, then bias in the other direction will also increase the type I error: 

```{r, warning=FALSE, echo=FALSE, fig.width = 5, fig.height=3.5, fig.align='center'}
Xbar_bias = 10

y1 = dnorm(x, Xbar_bias, SE)

df1 = data.frame(x = x, y = y1)

ggplot(data = df1, mapping = aes(x = x, y = y)) +
  geom_line(color = 'darkcyan') +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-5*SE, Xbar+4*SE)) +
  geom_line(data = df, aes(x = x, y = y)) +
  ylab('probability') + xlab(' ') +
  ggtitle(TeX('Type I Error if $\\mu = 12$ ($H_0$ true) under bias')) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='black', alpha=0.4) +
  geom_area(aes(x = ifelse(x > Xbar + Z_5*SE, x, 0)), fill='black', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_segment(x = Xbar_bias, xend = Xbar_bias, y = 0, yend = 0.5, color = 'darkcyan', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu_{H_0}$'), x = Xbar, y = 0.2, size = 5, color = 'black') +
  geom_text(label = TeX('$\\bar{X}$'), x = Xbar_bias, y = 0.2, size = 5, color = 'darkcyan') +
  geom_text(label = 'Type I Error', x = 8.8, y = -0.005, size = 2.5, color = 'black') +
  geom_text(label = 'Type I Error', x = 15.7, y = -0.005, size = 2.5, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'black') 
  
```
